name: 'Run Gemma 3 270M'
description: 'Runs Google Gemma 3 270M model inference using llama.cpp'
inputs:
  model:
    description: 'Model variant to use'
    required: true
    default: 'gemma-3-270m'
  query:
    description: 'Input text query for the model'
    required: true
  hf_token:
    description: 'Hugging Face Token (optional)'
    required: false
outputs:
  response:
    description: 'The output text generated by the model'
    value: ${{ steps.inference.outputs.response }}
runs:
  using: "composite"
  steps:
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Set HF Home
      shell: bash
      run: echo "HF_HOME=${{ github.workspace }}/hf_cache" >> $GITHUB_ENV

    - name: Cache Model
      id: cache-model
      uses: actions/cache@v4
      with:
        path: ${{ github.workspace }}/hf_cache
        key: gemma-3-270m-weights-${{ inputs.model }}
        restore-keys: |
          gemma-3-270m-weights-

    - name: Install Dependencies
      shell: bash
      run: |
        pip install -r ${{ github.action_path }}/requirements.txt

    - name: Run Inference
      id: inference
      shell: bash
      env:
        HF_TOKEN: ${{ inputs.hf_token }}
      run: |
        python ${{ github.action_path }}/run_inference.py \
          --model "${{ inputs.model }}" \
          --query "${{ inputs.query }}" \
          --hf_token "${{ inputs.hf_token }}"
