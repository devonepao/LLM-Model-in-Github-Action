name: 'Run Gemma 3 270M'
description: 'Runs Google Gemma 3 270M model inference using llama.cpp'
inputs:
  model:
    description: 'Model variant to use'
    required: true
    default: 'gemma-3-270m'
  query:
    description: 'Input text query for the model'
    required: true
  hf_token:
    description: 'Hugging Face Token (optional)'
    required: false
outputs:
  response:
    description: 'The output text generated by the model'
    value: ${{ steps.inference.outputs.response }}
runs:
  using: "composite"
  steps:
    - name: Set up uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
        cache-dependency-glob: "requirements.txt"

    - name: Set up Python
      shell: bash
      run: uv python install 3.10

    - name: Set HF Home
      shell: bash
      run: echo "HF_HOME=${{ github.workspace }}/hf_cache" >> $GITHUB_ENV

    - name: Cache Model
      id: cache-model
      uses: actions/cache@v4
      with:
        path: ${{ github.workspace }}/hf_cache
        key: gemma-3-weights-${{ inputs.model }}
        restore-keys: |
          gemma-3-weights-

    - name: Install HF CLI
      if: inputs.model == 'gemma-3-270m-it'
      shell: bash
      run: uv tool install huggingface_hub

    - name: Download Model (CLI)
      if: inputs.model == 'gemma-3-270m-it'
      shell: bash
      env:
        HF_TOKEN: ${{ inputs.hf_token }}
        HF_HOME: ${{ github.workspace }}/hf_cache
      run: |
        hf download google/gemma-3-270m-it --cache-dir $HF_HOME

    - name: Run Inference (with uv)
      id: inference
      shell: bash
      env:
        HF_TOKEN: ${{ inputs.hf_token }}
        HF_HOME: ${{ github.workspace }}/hf_cache
      run: |
        uv run --with-requirements requirements.txt ${{ github.action_path }}/run_inference.py \
          --model "${{ inputs.model }}" \
          --query "${{ inputs.query }}" \
          --hf_token "${{ env.HF_TOKEN }}"
