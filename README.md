# ü§ñ Run Gemma 3 (270M) in GitHub Actions

![GitHub Actions](https://img.shields.io/badge/github%20actions-%232671E5.svg?style=for-the-badge&logo=githubactions&logoColor=white)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![License](https://img.shields.io/github/license/harshityadav95/LLM-Model-in-Github-Action?style=for-the-badge)

A powerful, efficient GitHub Action to run **Google's Gemma 3 270M** model directly in your CI/CD pipelines. Optimized for standard GitHub-hosted runners using `llama.cpp` and `uv` for lightning-fast inference.

## üöÄ Capabilities

- **Zero-Config Inference**: Runs the ultra-compact Gemma 3 270M model out of the box.
- **Lightning Fast**: Built on `uv` for instant environment setup and `llama-cpp-python` for optimized CPU inference.
- **Smart Caching**: Automatically downloads and caches model weights (~300MB) using GitHub Actions Cache, making subsequent runs instant.
- **Secure**: Supports gated models via `HF_TOKEN` integration.
- **Cross-Platform**: Tested on both Ubuntu (AMD64) and ARM64 architecture runners.

## üõ†Ô∏è Usage

### Quick Start

Add this step to your workflow:

```yaml
- name: Run Gemma 3 Inference
  uses: harshityadav95/LLM-Model-in-Github-Action@main
  with:
    model: 'gemma-3-270m'
    query: 'Explain quantum computing in one sentence.'
    hf_token: ${{ secrets.HF_TOKEN }} # Required for gated models
```

### Full Workflow Example

Create `.github/workflows/ai-inference.yml`:

```yaml
name: AI Model Inference

on:
  workflow_dispatch:
    inputs:
      query:
        description: 'Input text for the model'
        required: true
        default: 'Hello world!'

jobs:
  run-model:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Run Gemma
        id: gemma
        uses: harshityadav95/LLM-Model-in-Github-Action@main
        with:
          model: 'gemma-3-270m-instruct'
          query: ${{ inputs.query }}
          hf_token: ${{ secrets.HF_TOKEN }}

      - name: View Result
        run: echo "Response: ${{ steps.gemma.outputs.response }}"
```

## ‚öôÔ∏è Configuration

| Input | Description | Required | Default |
|-------|-------------|----------|---------|
| `model` | Model variant to use. Options: `gemma-3-270m`, `gemma-3-270m-instruct`. | ‚úÖ | `gemma-3-270m` |
| `query` | The text prompt to send to the model. | ‚úÖ | - |
| `hf_token` | Hugging Face Access Token for downloading gated models. | ‚ùå | - |

| Output | Description |
|--------|-------------|
| `response` | The text generated by the model. |

## üîÆ Future Scope & Roadmap

We plan to expand this action's capabilities:

- [ ] **More Models**: Support for Gemma 1B/2B and other SLMs (TinyLlama, Phi-3).
- [ ] **Custom Models**: Allow users to provide any Hugging Face GGUF repo ID.
- [ ] **GPU Acceleration**: Optimize for self-hosted runners with GPU support.
- [ ] **Chat History**: Support multi-turn conversations for context-aware CI bots.
- [ ] **JSON Output**: structured output mode for programmatic usage in pipelines.

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1.  Fork the repository.
2.  Create your feature branch (`git checkout -b feature/AmazingFeature`).
3.  Commit your changes (`git commit -m 'Add some AmazingFeature'`).
4.  Push to the branch (`git push origin feature/AmazingFeature`).
5.  Open a Pull Request.

## üë• Contributors

- **Harshit Yadav** - *Initial Work* - [@harshityadav95](https://github.com/harshityadav95)

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
*Built with ‚ù§Ô∏è using [llama.cpp](https://github.com/abetlen/llama-cpp-python), [uv](https://github.com/astral-sh/uv), and Google Gemma.*
